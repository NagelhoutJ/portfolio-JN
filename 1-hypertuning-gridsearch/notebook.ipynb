{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises \n",
    "# 1. Tune the network\n",
    "Run the experiment below, explore the different parameters (see suggestions below) and study the result with tensorboard. \n",
    "Make a single page (1 a4) report of your findings. Use your visualisation skills to communicate your most important findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tomlserializer import TOMLSerializer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `tomlserializer` to easily keep track of our experiments, and to easily save the different things we did during our experiments.\n",
    "It can export things like settings and models to a simple `toml` file, which can be easily shared, checked and modified.\n",
    "\n",
    "First, we need the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-15 15:02:37.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\j.nagelhout\\.cache\\mads_datasets\\fashionmnist\u001b[0m\n",
      "\u001b[32m2025-09-15 15:02:37.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at C:\\Users\\j.nagelhout\\.cache\\mads_datasets\\fashionmnist\\fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to determine how well our model is performing. We will use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set up a single experiment.\n",
    "\n",
    "- We will show the model batches of 64 images, \n",
    "- and for every epoch we will show the model 100 batches (trainsteps=100).\n",
    "- then, we will test how well the model is doing on unseen data (teststeps=100).\n",
    "- we will report our results during training to tensorboard, and report all configuration to a toml file.\n",
    "- we will log the results into a directory called \"modellogs\", but you could change this to whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-15 15:02:37.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.settings\u001b[0m:\u001b[36mcheck_path\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mCreated logdir c:\\GitHub\\portfolio-JN\\1-hypertuning-gridsearch\\modellogs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a very basic model: a model with three linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    num_classes=10, units1=256, units2=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developped the `tomlserializer` package, it is a useful tool to save configs, models and settings as a tomlfile; that way it is easy to track what you changed during your experiments.\n",
    "\n",
    "This package will 1. check if there is a `__dict__` attribute available, and if so, it will use that to extract the parameters that do not start with an underscore, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True, 'num_classes': 10, 'units1': 256, 'units2': 256}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in model.__dict__.items() if not k.startswith(\"_\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if you want to add more parameters to the `.toml` file, eg `units3`, you can add them to the class like this:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int, units3: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.units3 = units3  # <-- add this line\n",
    "```\n",
    "\n",
    "And then it will be added to the `.toml` file. Check the result for yourself by using the `.save()` method of the `TomlSerializer` class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomlserializer = TOMLSerializer()\n",
    "tomlserializer.save(settings, \"settings.toml\")\n",
    "tomlserializer.save(model, \"model.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `settings.toml` and `model.toml` files to see what is in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `Trainer` class from my `mltrainer` module to train your model. It has the TOMLserializer integrated, so it will automatically save the settings and model to a toml file if you have added `TOML` as a reporttype in the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-15 15:02:37.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150237\u001b[0m\n",
      "\u001b[32m2025-09-15 15:02:56.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:00<00:00, 105.00it/s]\n",
      "\u001b[32m2025-09-15 15:02:58.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.9384 test 0.6679 metric ['0.7555']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:00<00:00, 145.62it/s]\n",
      "\u001b[32m2025-09-15 15:02:59.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.5898 test 0.5467 metric ['0.8006']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 100/100 [00:01<00:00, 72.27it/s]\n",
      "\u001b[32m2025-09-15 15:03:01.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.5015 test 0.5408 metric ['0.8055']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:04<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    ")\n",
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check in the modellogs directory the results of your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop this with a naive approach, called a grid-search (why do you think i call it naive?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units: 256, 256\n",
      "Units: 256, 128\n",
      "Units: 256, 64\n",
      "Units: 128, 256\n",
      "Units: 128, 128\n",
      "Units: 128, 64\n",
      "Units: 64, 256\n",
      "Units: 64, 128\n",
      "Units: 64, 64\n"
     ]
    }
   ],
   "source": [
    "units = [256, 128, 64]\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "        print(f\"Units: {unit1}, {unit2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this might not be the best way to search for a model; some configurations will be better than others (can you predict up front what will be the best configuration?).\n",
    "\n",
    "So, feel free to improve upon the gridsearch by adding your own logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-15 15:03:01.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150301\u001b[0m\n",
      "\u001b[32m2025-09-15 15:03:01.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 112.34it/s]\n",
      "\u001b[32m2025-09-15 15:03:10.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5040 test 0.4333 metric ['0.8434']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 114.27it/s]\n",
      "\u001b[32m2025-09-15 15:03:19.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3633 test 0.3960 metric ['0.8530']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 94.66it/s]\n",
      "\u001b[32m2025-09-15 15:03:30.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3296 test 0.3687 metric ['0.8662']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:28<00:00,  9.56s/it]\n",
      "\u001b[32m2025-09-15 15:03:30.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150330\u001b[0m\n",
      "\u001b[32m2025-09-15 15:03:30.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 151.33it/s]\n",
      "\u001b[32m2025-09-15 15:03:37.061\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5167 test 0.4132 metric ['0.8534']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 145.69it/s]\n",
      "\u001b[32m2025-09-15 15:03:44.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3700 test 0.3867 metric ['0.8603']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 105.96it/s]\n",
      "\u001b[32m2025-09-15 15:03:53.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3313 test 0.3511 metric ['0.8731']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:23<00:00,  7.82s/it]\n",
      "\u001b[32m2025-09-15 15:03:53.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150353\u001b[0m\n",
      "\u001b[32m2025-09-15 15:03:53.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 163.14it/s]\n",
      "\u001b[32m2025-09-15 15:03:59.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5352 test 0.4332 metric ['0.8425']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 127.13it/s]\n",
      "\u001b[32m2025-09-15 15:04:07.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3811 test 0.4166 metric ['0.8518']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 85.57it/s]\n",
      "\u001b[32m2025-09-15 15:04:19.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3430 test 0.3689 metric ['0.8698']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:26<00:00,  8.68s/it]\n",
      "\u001b[32m2025-09-15 15:04:19.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150419\u001b[0m\n",
      "\u001b[32m2025-09-15 15:04:19.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 97.79it/s]\n",
      "\u001b[32m2025-09-15 15:04:30.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5305 test 0.4164 metric ['0.8500']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 89.28it/s]\n",
      "\u001b[32m2025-09-15 15:04:41.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3799 test 0.4019 metric ['0.8597']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:11<00:00, 82.68it/s]\n",
      "\u001b[32m2025-09-15 15:04:53.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3380 test 0.3588 metric ['0.8676']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:33<00:00, 11.25s/it]\n",
      "\u001b[32m2025-09-15 15:04:53.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150453\u001b[0m\n",
      "\u001b[32m2025-09-15 15:04:53.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 118.92it/s]\n",
      "\u001b[32m2025-09-15 15:05:01.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5387 test 0.4486 metric ['0.8342']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 108.47it/s]\n",
      "\u001b[32m2025-09-15 15:05:11.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3842 test 0.3986 metric ['0.8533']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 104.12it/s]\n",
      "\u001b[32m2025-09-15 15:05:20.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3436 test 0.3799 metric ['0.8586']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:27<00:00,  9.06s/it]\n",
      "\u001b[32m2025-09-15 15:05:20.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150520\u001b[0m\n",
      "\u001b[32m2025-09-15 15:05:20.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 95.43it/s] \n",
      "\u001b[32m2025-09-15 15:05:31.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5542 test 0.4649 metric ['0.8329']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 108.39it/s]\n",
      "\u001b[32m2025-09-15 15:05:40.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.4000 test 0.4298 metric ['0.8440']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 113.78it/s]\n",
      "\u001b[32m2025-09-15 15:05:49.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3548 test 0.3800 metric ['0.8642']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:28<00:00,  9.56s/it]\n",
      "\u001b[32m2025-09-15 15:05:49.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150549\u001b[0m\n",
      "\u001b[32m2025-09-15 15:05:49.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 98.94it/s] \n",
      "\u001b[32m2025-09-15 15:05:59.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5420 test 0.4185 metric ['0.8474']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:11<00:00, 84.14it/s]\n",
      "\u001b[32m2025-09-15 15:06:11.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3789 test 0.3956 metric ['0.8536']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 148.56it/s]\n",
      "\u001b[32m2025-09-15 15:06:18.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3477 test 0.3946 metric ['0.8560']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:29<00:00,  9.78s/it]\n",
      "\u001b[32m2025-09-15 15:06:18.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150618\u001b[0m\n",
      "\u001b[32m2025-09-15 15:06:18.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 136.14it/s]\n",
      "\u001b[32m2025-09-15 15:06:26.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5686 test 0.4538 metric ['0.8401']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 110.10it/s]\n",
      "\u001b[32m2025-09-15 15:06:35.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3987 test 0.4140 metric ['0.8481']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 142.71it/s]\n",
      "\u001b[32m2025-09-15 15:06:42.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3481 test 0.3848 metric ['0.8637']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:23<00:00,  7.98s/it]\n",
      "\u001b[32m2025-09-15 15:06:42.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250915-150642\u001b[0m\n",
      "\u001b[32m2025-09-15 15:06:42.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 102.72it/s]\n",
      "\u001b[32m2025-09-15 15:06:52.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5827 test 0.4603 metric ['0.8353']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 105.72it/s]\n",
      "\u001b[32m2025-09-15 15:07:01.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.4071 test 0.4272 metric ['0.8418']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 133.57it/s]\n",
      "\u001b[32m2025-09-15 15:07:09.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3600 test 0.3947 metric ['0.8559']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [00:26<00:00,  8.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "units = [256, 128, 64]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "\n",
    "        model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "        )\n",
    "        trainer.loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-16 22:10:03.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221003\u001b[0m\n",
      "\u001b[32m2025-09-16 22:10:03.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 253.28it/s]\n",
      "\u001b[32m2025-09-16 22:10:07.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5932 test 0.4650 metric ['0.8408']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 255.62it/s]\n",
      "\u001b[32m2025-09-16 22:10:11.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.4099 test 0.4251 metric ['0.8470']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 235.86it/s]\n",
      "\u001b[32m2025-09-16 22:10:16.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3685 test 0.3999 metric ['0.8541']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 247.25it/s]\n",
      "\u001b[32m2025-09-16 22:10:20.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3435 test 0.3740 metric ['0.8688']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 234.50it/s]\n",
      "\u001b[32m2025-09-16 22:10:24.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3262 test 0.3945 metric ['0.8596']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:10:24.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3740, current loss 0.3945.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 226.84it/s]\n",
      "\u001b[32m2025-09-16 22:10:29.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.3099 test 0.3454 metric ['0.8738']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 231.79it/s]\n",
      "\u001b[32m2025-09-16 22:10:33.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2995 test 0.3548 metric ['0.8767']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:10:33.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3454, current loss 0.3548.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 236.93it/s]\n",
      "\u001b[32m2025-09-16 22:10:37.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2860 test 0.3544 metric ['0.8734']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:10:37.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3454, current loss 0.3544.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:33<00:00,  4.23s/it]\n",
      "\u001b[32m2025-09-16 22:10:37.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221037\u001b[0m\n",
      "\u001b[32m2025-09-16 22:10:37.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 278.50it/s]\n",
      "\u001b[32m2025-09-16 22:10:41.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5709 test 0.4394 metric ['0.8402']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 242.22it/s]\n",
      "\u001b[32m2025-09-16 22:10:45.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.4051 test 0.4305 metric ['0.8475']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 214.20it/s]\n",
      "\u001b[32m2025-09-16 22:10:50.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3596 test 0.3890 metric ['0.8581']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 203.42it/s]\n",
      "\u001b[32m2025-09-16 22:10:55.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3379 test 0.3876 metric ['0.8610']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 195.14it/s]\n",
      "\u001b[32m2025-09-16 22:11:00.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3206 test 0.3661 metric ['0.8608']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 195.39it/s]\n",
      "\u001b[32m2025-09-16 22:11:05.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.3028 test 0.3459 metric ['0.8755']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 187.23it/s]\n",
      "\u001b[32m2025-09-16 22:11:11.219\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2946 test 0.3527 metric ['0.8728']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:11:11.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3459, current loss 0.3527.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 182.18it/s]\n",
      "\u001b[32m2025-09-16 22:11:16.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2810 test 0.3458 metric ['0.8777']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:38<00:00,  4.87s/it]\n",
      "\u001b[32m2025-09-16 22:11:16.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221116\u001b[0m\n",
      "\u001b[32m2025-09-16 22:11:16.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 247.08it/s]\n",
      "\u001b[32m2025-09-16 22:11:20.869\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5562 test 0.4618 metric ['0.8275']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 239.38it/s]\n",
      "\u001b[32m2025-09-16 22:11:25.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3929 test 0.4000 metric ['0.8537']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 215.62it/s]\n",
      "\u001b[32m2025-09-16 22:11:29.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3514 test 0.3845 metric ['0.8584']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 213.71it/s]\n",
      "\u001b[32m2025-09-16 22:11:34.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3265 test 0.3783 metric ['0.8619']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 206.83it/s]\n",
      "\u001b[32m2025-09-16 22:11:39.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3103 test 0.3565 metric ['0.8716']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 205.27it/s]\n",
      "\u001b[32m2025-09-16 22:11:44.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2939 test 0.3409 metric ['0.8761']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 174.18it/s]\n",
      "\u001b[32m2025-09-16 22:11:50.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2820 test 0.3408 metric ['0.8796']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 165.72it/s]\n",
      "\u001b[32m2025-09-16 22:11:56.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2716 test 0.3491 metric ['0.8746']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:11:56.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3408, current loss 0.3491.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:39<00:00,  4.94s/it]\n",
      "\u001b[32m2025-09-16 22:11:56.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221156\u001b[0m\n",
      "\u001b[32m2025-09-16 22:11:56.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 263.28it/s]\n",
      "\u001b[32m2025-09-16 22:12:00.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5545 test 0.4512 metric ['0.8397']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 246.36it/s]\n",
      "\u001b[32m2025-09-16 22:12:04.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3904 test 0.4056 metric ['0.8539']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 213.82it/s]\n",
      "\u001b[32m2025-09-16 22:12:09.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3484 test 0.4017 metric ['0.8521']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 198.44it/s]\n",
      "\u001b[32m2025-09-16 22:12:14.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3270 test 0.3562 metric ['0.8723']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 194.31it/s]\n",
      "\u001b[32m2025-09-16 22:12:19.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3033 test 0.3544 metric ['0.8741']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 196.19it/s]\n",
      "\u001b[32m2025-09-16 22:12:24.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2924 test 0.3384 metric ['0.8787']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 184.12it/s]\n",
      "\u001b[32m2025-09-16 22:12:30.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2767 test 0.3809 metric ['0.8644']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:12:30.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3384, current loss 0.3809.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 182.63it/s]\n",
      "\u001b[32m2025-09-16 22:12:35.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2655 test 0.3290 metric ['0.8786']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:39<00:00,  4.93s/it]\n",
      "\u001b[32m2025-09-16 22:12:35.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221235\u001b[0m\n",
      "\u001b[32m2025-09-16 22:12:35.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 229.32it/s]\n",
      "\u001b[32m2025-09-16 22:12:40.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5469 test 0.4356 metric ['0.8416']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 209.48it/s]\n",
      "\u001b[32m2025-09-16 22:12:45.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3833 test 0.4026 metric ['0.8549']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 185.50it/s]\n",
      "\u001b[32m2025-09-16 22:12:50.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3484 test 0.3635 metric ['0.8703']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 178.92it/s]\n",
      "\u001b[32m2025-09-16 22:12:56.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3192 test 0.3639 metric ['0.8708']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:12:56.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3635, current loss 0.3639.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 177.03it/s]\n",
      "\u001b[32m2025-09-16 22:13:01.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.3023 test 0.3569 metric ['0.8707']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 182.18it/s]\n",
      "\u001b[32m2025-09-16 22:13:07.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2885 test 0.3524 metric ['0.8756']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 163.40it/s]\n",
      "\u001b[32m2025-09-16 22:13:13.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2750 test 0.3602 metric ['0.8731']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:13:13.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3524, current loss 0.3602.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 167.42it/s]\n",
      "\u001b[32m2025-09-16 22:13:19.633\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2635 test 0.3334 metric ['0.8813']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:43<00:00,  5.48s/it]\n",
      "\u001b[32m2025-09-16 22:13:19.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221319\u001b[0m\n",
      "\u001b[32m2025-09-16 22:13:19.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 225.62it/s]\n",
      "\u001b[32m2025-09-16 22:13:24.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5331 test 0.4259 metric ['0.8477']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 213.73it/s]\n",
      "\u001b[32m2025-09-16 22:13:28.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3743 test 0.3809 metric ['0.8629']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 191.71it/s]\n",
      "\u001b[32m2025-09-16 22:13:34.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3365 test 0.3788 metric ['0.8642']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 179.94it/s]\n",
      "\u001b[32m2025-09-16 22:13:39.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3129 test 0.3524 metric ['0.8728']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 173.51it/s]\n",
      "\u001b[32m2025-09-16 22:13:45.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2915 test 0.3517 metric ['0.8699']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 157.19it/s]\n",
      "\u001b[32m2025-09-16 22:13:51.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2771 test 0.3469 metric ['0.8755']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 143.54it/s]\n",
      "\u001b[32m2025-09-16 22:13:58.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2692 test 0.3443 metric ['0.8777']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 127.77it/s]\n",
      "\u001b[32m2025-09-16 22:14:06.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2548 test 0.3294 metric ['0.8850']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:47<00:00,  5.91s/it]\n",
      "\u001b[32m2025-09-16 22:14:06.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221406\u001b[0m\n",
      "\u001b[32m2025-09-16 22:14:06.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 157.90it/s]\n",
      "\u001b[32m2025-09-16 22:14:13.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5305 test 0.4253 metric ['0.8473']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 145.45it/s]\n",
      "\u001b[32m2025-09-16 22:14:20.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3782 test 0.4004 metric ['0.8562']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 120.63it/s]\n",
      "\u001b[32m2025-09-16 22:14:28.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3391 test 0.3676 metric ['0.8667']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 116.56it/s]\n",
      "\u001b[32m2025-09-16 22:14:37.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3134 test 0.3762 metric ['0.8633']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:14:37.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3676, current loss 0.3762.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 111.13it/s]\n",
      "\u001b[32m2025-09-16 22:14:46.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2981 test 0.3484 metric ['0.8756']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 110.99it/s]\n",
      "\u001b[32m2025-09-16 22:14:56.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2812 test 0.3262 metric ['0.8816']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 101.77it/s]\n",
      "\u001b[32m2025-09-16 22:15:05.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2695 test 0.3540 metric ['0.8722']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:15:05.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3262, current loss 0.3540.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 103.69it/s]\n",
      "\u001b[32m2025-09-16 22:15:15.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2548 test 0.3441 metric ['0.8768']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:15:15.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3262, current loss 0.3441.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [01:08<00:00,  8.60s/it]\n",
      "\u001b[32m2025-09-16 22:15:15.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221515\u001b[0m\n",
      "\u001b[32m2025-09-16 22:15:15.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:03<00:00, 238.98it/s]\n",
      "\u001b[32m2025-09-16 22:15:19.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5158 test 0.4968 metric ['0.8114']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 212.57it/s]\n",
      "\u001b[32m2025-09-16 22:15:24.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3748 test 0.3824 metric ['0.8599']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 176.11it/s]\n",
      "\u001b[32m2025-09-16 22:15:30.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3322 test 0.3658 metric ['0.8670']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 154.22it/s]\n",
      "\u001b[32m2025-09-16 22:15:37.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3092 test 0.3582 metric ['0.8730']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 146.37it/s]\n",
      "\u001b[32m2025-09-16 22:15:44.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2953 test 0.3633 metric ['0.8691']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:15:44.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3582, current loss 0.3633.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 139.48it/s]\n",
      "\u001b[32m2025-09-16 22:15:51.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2768 test 0.3296 metric ['0.8825']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 120.17it/s]\n",
      "\u001b[32m2025-09-16 22:15:59.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2603 test 0.3395 metric ['0.8796']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:15:59.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3296, current loss 0.3395.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 117.07it/s]\n",
      "\u001b[32m2025-09-16 22:16:08.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2530 test 0.3201 metric ['0.8844']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [00:52<00:00,  6.62s/it]\n",
      "\u001b[32m2025-09-16 22:16:08.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to modellogs\\20250916-221608\u001b[0m\n",
      "\u001b[32m2025-09-16 22:16:08.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:04<00:00, 194.76it/s]\n",
      "\u001b[32m2025-09-16 22:16:13.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 0.5083 test 0.4276 metric ['0.8452']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:05<00:00, 161.64it/s]\n",
      "\u001b[32m2025-09-16 22:16:20.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 0.3689 test 0.3826 metric ['0.8612']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 129.17it/s]\n",
      "\u001b[32m2025-09-16 22:16:27.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 0.3322 test 0.3790 metric ['0.8632']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 122.08it/s]\n",
      "\u001b[32m2025-09-16 22:16:36.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 0.3010 test 0.3756 metric ['0.8688']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 109.74it/s]\n",
      "\u001b[32m2025-09-16 22:16:45.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 0.2911 test 0.3347 metric ['0.8815']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 108.28it/s]\n",
      "\u001b[32m2025-09-16 22:16:54.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 0.2753 test 0.3380 metric ['0.8775']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:16:54.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3347, current loss 0.3380.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 105.58it/s]\n",
      "\u001b[32m2025-09-16 22:17:03.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 0.2612 test 0.3268 metric ['0.8783']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 97.04it/s]\n",
      "\u001b[32m2025-09-16 22:17:14.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 0.2474 test 0.3428 metric ['0.8792']\u001b[0m\n",
      "\u001b[32m2025-09-16 22:17:14.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 0.3268, current loss 0.3428.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 8/8 [01:05<00:00,  8.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "units = [64, 128, 256]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=8,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "\n",
    "        model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "        )\n",
    "        trainer.loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have set the ReportType to TOML, you will find in every log dir a model.toml and settings.toml file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, and study the result with tensorboard. \n",
    "\n",
    "Locally, it is easy to do that with VS code itself. On the server, you have to take these steps:\n",
    "\n",
    "- in the terminal, `cd` to the location of the repository\n",
    "- activate the python environment for the shell. Note how the correct environment is being activated.\n",
    "- run `tensorboard --logdir=modellogs` in the terminal\n",
    "- tensorboard will launch at `localhost:6006` and vscode will notify you that the port is forwarded\n",
    "- you can either press the `launch` button in VScode or open your local browser at `localhost:6006`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
